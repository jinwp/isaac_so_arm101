# Copyright (c) 2024-2025, Muammer Bay (LycheeAI), Louis Le Lay
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause
#
# Copyright (c) 2022-2026, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

seed: 42

# =============================================================================
# SAC for Lift Task — Tuned for off-policy training
# =============================================================================
# IMPORTANT: Run with fewer parallel envs than PPO! Use:
#   --num_envs 128
# Example:
#   python -m isaaclab.train --task Isaac-SO-ARM101-Lift-Cube-v0 \
#       --algorithm SAC --num_envs 128
#
# Why? SAC updates the network at EVERY timestep (unlike PPO which batches
# rollouts). With 4096 envs, each timestep triggers a heavy gradient update
# causing the "move-stop-move" lagging behavior.
# =============================================================================

models:
  separate: True
  policy:
    class: GaussianMixin
    clip_actions: False
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.5
    network:
      - name: net
        input: OBSERVATIONS
        layers: [256, 128, 64]
        activations: elu
    output: tanh(ACTIONS)
  critic_1:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128, 64]
        activations: elu
    output: ONE
  critic_2:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128, 64]
        activations: elu
    output: ONE
  target_critic_1:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128, 64]
        activations: elu
    output: ONE
  target_critic_2:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128, 64]
        activations: elu
    output: ONE

memory:
  class: RandomMemory
  # With 128 envs: buffer holds 1,000,000 / 128 ≈ 7812 timesteps of data
  # (~31 full episodes worth). This provides proper temporal decorrelation.
  memory_size: 1000000

agent:
  class: SAC
  gradient_steps: 1
  # Standard SAC batch size (was 4096 → way too large, slowed each update)
  batch_size: 256
  discount_factor: 0.99
  polyak: 0.005
  learning_rate: [3.0e-4, 3.0e-4, 3.0e-4]
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs: null
  # With 128 envs: 10000 timesteps × 128 = 1.28M random transitions
  # More diverse exploration before learning starts
  random_timesteps: 10000
  learning_starts: 10000
  grad_norm_clip: 1.0
  learn_entropy: True
  initial_entropy_value: 0.5
  # action_dim ≈ 6 (5 arm + 1 gripper), less restrictive for more exploration
  target_entropy: -3.0
  rewards_shaper_scale: 1.0
  # logging and checkpoint
  experiment:
    directory: "soarm_lift"
    experiment_name: "exploration_fix"
    write_interval: auto
    checkpoint_interval: auto

trainer:
  class: SequentialTrainer
  # With 128 envs and 500K timesteps:
  #   Total env interactions = 500,000 × 128 = 64M (vs PPO's 48K × 4096 = 197M)
  #   Total gradient updates = ~495,000
  # SAC is more sample-efficient so fewer total interactions are fine.
  timesteps: 500000
  environment_info: log
