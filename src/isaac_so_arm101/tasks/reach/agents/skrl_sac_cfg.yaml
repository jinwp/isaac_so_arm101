# Copyright (c) 2024-2025, Muammer Bay (LycheeAI), Louis Le Lay
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause
#
# Copyright (c) 2022-2026, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

seed: 42

models:
  separate: True
  policy:
    class: GaussianMixin
    clip_actions: False
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.3  # Increased exploration vs 0.0
    network:
      - name: net
        input: OBSERVATIONS
        layers: [256, 128]  # Balanced size for GPU memory
        activations: elu
    output: tanh(ACTIONS)
  critic_1:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128]  # Balanced size for GPU memory
        activations: elu
    output: ONE
  critic_2:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128]  # Balanced size for GPU memory
        activations: elu
    output: ONE
  target_critic_1:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128]  # Balanced size for GPU memory
        activations: elu
    output: ONE
  target_critic_2:
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: concatenate([OBSERVATIONS, ACTIONS])
        layers: [256, 128]  # Balanced size for GPU memory
        activations: elu
    output: ONE

memory:
  class: RandomMemory
  memory_size: 50000  # Balanced buffer size for GPU memory

agent:
  class: SAC
  gradient_steps: 2  # Standard for vectorized envs to avoid memory issues
  batch_size: 64  # Reduced batch size for GPU memory
  discount_factor: 0.98  # Slightly lower to prevent Q-value overestimation
  polyak: 0.005
  learning_rate: [3.0e-4, 3.0e-4, 3.0e-4]  # Uniform moderate LR
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs: null
  random_timesteps: 1000
  learning_starts: 1000
  grad_norm_clip: 1.0
  learn_entropy: True
  initial_entropy_value: 0.1  # Lower initial entropy to prevent Q-value explosion
  target_entropy: -3.0  # Tighter entropy (action_dim=5 â†’ -dim is common default)
  rewards_shaper_scale: 10.0  # Scale up rewards to dominate entropy bonus
  # logging and checkpoint
  experiment:
    directory: "soarm_reach"
    experiment_name: "sac-newnew"
    write_interval: auto
    checkpoint_interval: auto

trainer:
  class: SequentialTrainer
  timesteps: 24000
  environment_info: log
